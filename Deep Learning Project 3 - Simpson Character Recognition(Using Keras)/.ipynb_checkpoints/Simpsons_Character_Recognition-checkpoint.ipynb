{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64b3eeb",
   "metadata": {},
   "source": [
    "# The Simpsons Character Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1bdd1",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f9fcd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPool2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643422c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 20 # number of folders/classes for characters\n",
    "img_row, img_col = 32, 32 # the dimensions of thee images to be used\n",
    "batch_size = 16 # number of images trained per epoch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c583fff",
   "metadata": {},
   "source": [
    "### Directories for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb9680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = r''\n",
    "val_data_dir = r''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30036fe",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed26654",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageGenerator(\n",
    "        rescale = 1./255,# scales down the pixels from 0-255 to 0 and 1 so they are suited for neural network\n",
    "        rotation_range = 30, # allows to rotate the image in the degree specified\n",
    "        width_shift_range = 0.3, # floating point the image will be shifted to the right or to the left\n",
    "        height_shift_range = 0.3, # floating point the image will be shifted to the up or to the down\n",
    "        horizontal_flip = True, # it flips the rows and columns \n",
    "        fill_mode = 'nearest') # since when rotating the image some pixels will move outsied the  picture and leave empty pixels\n",
    "# the fill mode will fill them in by using the nearest\n",
    "\n",
    "val_datagen = ImageGenerator(rescale = 1./255) # scale down or normalize the images of validation\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,  \n",
    "        target_size = (img_row, img_col), \n",
    "        batch_size = batch_size, \n",
    "        class_mode = 'categorical') \n",
    "\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "        train_data_dir,  \n",
    "        target_size = (img_row, img_col), \n",
    "        batch_size = batch_size, \n",
    "        class_mode = 'categorical') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a14101",
   "metadata": {},
   "source": [
    "### Build VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ab8138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a seuquential model\n",
    "# Block 1\n",
    "model = Sequential()\n",
    "model.add(Conv2D(input_shape=(32, 32, 3), filters = 64, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size = (2, 2), strides = (2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#block 2\n",
    "model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(filters = 128, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size = (2, 2), strides = (2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#block 3\n",
    "model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(filters = 256, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size = (2, 2), strides = (2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#block 4\n",
    "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size = (2, 2), strides = (2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# block 5\n",
    "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(filters = 512, kernel_size = (3,3), padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPool2D(pool_size = (2, 2), strides = (2,2)))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Dense layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units = 4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units = 20, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf2cdaa",
   "metadata": {},
   "source": [
    "### Visualization of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5837be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras\n",
    "from keras.models import Sequential\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "\n",
    "plot_model(model, to_file = 'myvgg16.png', show_shapes = True, show_layer_names = True)\n",
    "img = mpimg.imread('myvgg16.png')\n",
    "plt.figure(figsize = (100, 70))\n",
    "imgplot = plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa5f905",
   "metadata": {},
   "source": [
    "### Train VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9533b042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(r'\\\\simpsons_model.h5',\n",
    "                            monitor = 'val_losss',\n",
    "                            mode = 'min',\n",
    "                            save_best_only = True,\n",
    "                            verbose=1)\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor ='val_loss',\n",
    "                             min_delta = 0,\n",
    "                             patience = 3,\n",
    "                             verbose = 1,\n",
    "                             restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor ='val_loss',\n",
    "                             factor = 0.2,\n",
    "                             patience = 3,\n",
    "                             verbose = 1,\n",
    "                             min_delta = 0.00001)\n",
    "\n",
    "callbacks = [checkpoint, earlyStopping, reduce_lr]\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "             optimizer = Adam(lr = 0.001),\n",
    "             metrics = ['accuracy'])\n",
    "\n",
    "num_of_train_samples = 19548\n",
    "num_of_val_samples = 990\n",
    "epochs = 20\n",
    "\n",
    "history = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch = num_of_train_samples//batch_size, \n",
    "            epochs = epochs,\n",
    "            callbacks = callbacks,\n",
    "            validation_data = val_generator,\n",
    "            validation_steps = num_of_val_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fafeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "classifier = load_model(r'simpsons_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b5eb2a",
   "metadata": {},
   "source": [
    "### Testing using a single character picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37537cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "img = image.load_img(r'', target_size = (32, 32))\n",
    "img =np.asarray(img)\n",
    "plt.imshow(img)\n",
    "img = np.expand_dims(img, axis = 1)\n",
    "\n",
    "# create classes for characters\n",
    "classes = r'' # the directory to all the 20 characters\n",
    "my_dir = [d for d in os.listdir(classes) if os.path.isdir(os.path.join(classes, d))]\n",
    "\n",
    "pred_character = classifier.predict_classes(img)[0]\n",
    "plt.title(my_dir[pred_character])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcabc2d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528d8ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb36aac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaea13b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a557c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e0cfbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
